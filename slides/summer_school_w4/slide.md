---
_title: Summer School Week4
addons:
  - slidev-addon-addon
---

# Summer School Week4

洪晙宸

---
layout: cover
---

# Introduction & How do PLM works

---

# Framework of Pre-training

![](/Framework_of_pretraining.png)

---

# Pre-training for NLP

![](/Pretraining_for_nlp.png){width=470px}

- Use the benefit form pre-traing parameters so we don't have to traing the whole model

---

# Contextualized Word Representations

- The tokens with similar meaning have similar embedding.

![](/Contextualized_Word_Representations.png){width=650px}

---

# BERTology - What does each layer learn?

- Higher classifier accuracy does not always mean encoding more information.
- BERT understand a sentance order: Surface => Sytavtic => Semantic
  - There are no a clear line of each step, they may be mixed.

---

# Analyzing what BERT learned during training

- BERT Embryology: When does BERT know POS tagging, syntactic parsing, semantics?
- When Do You Need Billions of Words of Pretraining Data?

---

# Cross-discipline Capability

- Use human language pretrained model to do not human languages tasks, like DNA classification

<div class="grid grid-cols-2 gap-2">

![](/Cross_discipline_Capability.png)

![](/Cross_discipline_Capability_Result.png)

</div>

- The pretrained models learn some general skills for the classification

---

# Pre-training on Artificial Data

- By generating artificial data with different rules, we can know
what are the key factors for the success of pre-training.

![](/Pretraining_on_Artificial_Data.png){width=550px}

---

# Pre-training on Artificial Data

- English: The upper bound
- Random: Very low, which means [data plays the role]{color=red}
- Paired: Structured data is critical for [learning useful skills for NLP]{color=red}
- Shuffle: [Long-range reading]{color=red} may be the key to the success of a pretrained model?
- Learning to read a long-range in a sequence is crucial. 

![](/Pretraining_on_Artificial_Data_Improvements.png){width=180px}

---
layout: cover
---

# How to use PLMs: Contrastive Learning for PLMs

---

# Contrastive Learning

- Similar inputs have similar representations -> [Positive Pairs]{color=green}
- Dissimilar inputs have dissimilar representations. -> [Negative Pairs]{color=red}
- When apply on NLP:
  - eg: `How is the [MASK] today ?`
  - [Positive]{color=green}: non-contextualized representation of “weather”
  - [Negative]{color=red}: non-contextualized representation of all the other words in the vocabulary
- **Sentence-level task!**{color=yellow}: We have infinite possible sentences; not possible to enumerate all the sentences in the world.
  - Good to apply contrastive learning for sentence-level representations.

---

# Why we need sentence-level representations?

- Provide as a **backbone** that can be useful on a variety of downstream sentence-level tasks
- Good generalization ability on tasks without much training data
  - e.g. even **linear probing** can achieve good performance
- Efficient sentence-level **clustering** or **semantic search** by inner products
- Measure similarities among sentence pairs
- Unsupervised methods are more desirable in order to be applied to languages beyond English

---

# How to obtain sentence-level representations from BERTs?

- It cannot be trivially obtained from token-level representations
- Average pooling performs even worse than avg. GloVe embeddings
- **Representation degeneration**: the learned embeddings occupy a narrow cone in the vector space
  - Limits the expressiveness of the vector space

---

# Post-processing Methods

- BERT-flow: map to a smooth isotropic semantic space
- BERT-whitening

![](/BERT_post_process_result.png){width=700px}

---

# Contrastive Learning Methods: Designed Positives

<div class="grid grid-cols-2 gap-4">
<div>

- **DeCLUTR**
  - **Positive**: Overlapping/adjacent spans from the same document
  - **Negative**: hard negatives from same docs, easy negatives from different docs

![](/DeCLUTR.png){width=250px}

</div>
<div>

- **ConSERT**
  - All the possible augmentations on token embedding space

![](/ConSERT.png)

</div>
</div>

---

# Contrastive Learning Methods: Generating Positives

- Continuations generated by GPT-2 XL

![](/DINO.png){width=400px}

---

# Contrastive Learning Methods: Bootstrapping Methods

<div class="grid grid-cols-2 gap-4">
<div>

- BYOL
  - Not contrastive learning
  - Only positive pairs, no negatives pairs
  - Use a **moving average target network** to prevent mode collapsing


</div>

![](/BYOL.png)

</div>

---

# Contrastive Learning Methods: Dropout Augmentations

- SimCSE: Using different dropout masks (in Transformer layers) as augmentation
  - Model architecture is the same

<div class="flex gap-2">

![](/SimCSE_unsupervised.png){width=300px}

![](/SimCSE_supervised.png){width=405px}

</div>

- mSimCSE: Contrastive learning on **only English** data with multilingual models (mBERT, XLM-R) can align all other other languages **without any parallel data**. 

---

# Contrastive Learning Methods: Equivariant Contrastive Learning

- DiffCSE

![](/DiffCSE.png){width=550px}

---

# Contrastive Learning Methods: Prompting

- PromptBERT
  - Design/search good prompt templates to better extract sentence embeddings from BERT without fine-tuning
  - Further fine-tuning with contrastive loss:
    - Using sentence vectors produced by two different templates as a positive pair

---

# Contrastive Learning Methods: Ranking-based Methods

- RankEncoder
  - Refine the vector space of existing models like SimCSE, PromptBERT
  - Leverage ranking information from the **whole corpus**
  - Train a new encoder to match the cosine similarity of rank vectors
  - RankEncoder can be aware of the fine-grain interaction between the **similar sentences** in the corpus
  - Closing the gap between unsupervised and supervised sentence representations

---
layout: cover
---

# How to use PLMs: Parameter-efficient fine-tuning

---

# Why Parameter-efficient fine-tuning

- Problem: PLMs are gigantic (in terms of numbers of parameters, model size, and the storage needed to store the model)
- Solution: Reduce the number of parameters by parameter-efficient fine-tuning
- Comparison:
  - Standard Fine-tuning: Directly modify hidden representations => Gigantic
  - Parameter-Efficient Fine-tuning: Adds on hidden representations, can choose where to add. The original parmeter will stay still

---

# PLM: Adapter

- Add after Multi-head Attention and Feed-Forward Layer

![](/Adapter.png){width=80%}

---

# PLM: LORA

- Add on Feed-Forward Layer

![](/LORA.png){width=80%}

---

# PLM: Prefix Tuning

- Insert trainable prefix in each layer

![](/Prefix_Tuning.png){width=70%}

---

# PLM: Prompt Tuning

- Soft Prompting: Prepend the prefix embedding at the input layer

![](/Soft_Prompt_Tuning.png){width=70%}

---

# PLM: Prompt Tuning

- Hard Prompting: add words in the input sentence

![](/Hard_Prompt_Tuning.png){width=70%}

---

# PLM: Benefits

- Drastically decreases the task-specific parameters
- Less easier to overfit on training data; better out-of-domain performance
- Fewer parameters to fine-tune, making them good candidates when training with small dataset

![](/PLM_Reduced_Parameters.png){width=70%}

---
layout: cover
---

# How do PLMs work: Using PLMs with different amounts of data

---

# Intermediate-task fine-tuning

- Transfer the knowledge from a model finetuned on other tasks
- Conclusions: 
  - Same type of tasks is the most beneficial
  - Even when the intermediate-task or the target task has limited data
  - **Soft Prompt Transfer (SPoT)**:
    - When fine-tuning with soft prompt tuning, we only need to transfer the prompt embedding instead of the whole model
    - The soft prompt of a task can be used as the task embedding of that task

---

# Multi-task fine-tuning

- Fine-tune the PLM using the auxiliary task datasets and the target task dataset simultaneously

---

# Prompt tuning for few-shot learning

- Pprompt template: convert data points into a natural language prompt
- PLM: perform language modeling
- Verbalizer: A mapping between the label and the vocabulary

![](/Prompt_Tuning_for_few-shot_learning.png){width=70%}

---

# Prompt tuning for few-shot learning

- Prompt tuning has better performance under data scarcity because
  - It incorporates human knowledge
  - It introduces no new parameters
- How to select the verbalizer?
  1. **Manual design**: require task-specific knowledge
  2. **Prototypical verbalizer**: use learnable prototype vectors to represent a class, instead of using the words in the vocabulary
- Improve: LM-BFF
  ![](/LM-BFF.png){width=500px}

---

# Semi-supervised learning with PLMs

- Use the labeled data to train a good model and use that
model to label the unlabeled data
- Pattern-Exploiting Training (PET)
  - Use different prompts and verbalizer to prompt-tune different PLMs on the labeled dataset
  - Predict the unlabeled dataset and combine the predictions from different models
  - Use a PLM with classifier head to train on the soft-labeled data set
  ![](/PET.png){width=80%}

---

# Semi-supervised learning with PLMs

- STraTa: Self-Training with Task Augmentation
  - Use unlabeled data to generate an NLI dataset, and finetuned on the NLI dataset as the intermediate task to obtain the base model

<div class="grid grid-cols-2 gap-2">
<div class="text-size-base">

### **Steps**

1. Train an NLI data generator using another labeled NLI dataset using a generative language model
2. Use the trained data generator to generate NLI dataset using the in-domain unlabeled data
3. Use the generated in-domain NLI dataset to fine-tune an NLI model. The finetuned model is used to initialize the teacher model and student model in self-training

</div>

![](/STraTa.png)

</div>

---

# Zero-shot learning

- During pre-training, the training datasets implicitly contains a mixture of different tasks
- Encoder-decoder model pretrained using MLM is the best

---
layout: cover
---

# Evaluating LLM-based Applications

---

# Why evaluation?

- Models are constantly updating
- LLMs makes tons of mistakes
- New prompt looks better in a few examples, but may not be better in general
- It helps:
  - Validation that model avoids common failure modes
  - Common language of go/not go decisions
  - Roadmap for improvements to model performance

---

# What makes evaluation difficult?

- Trained on the internet => drift
- Qualitative => hard to measure success
- Diversity of behaviors

---

# The problem with benchmarks

- Benchmark doesn't work on your case
- Doesn't include prompting, ICL, finetune etc.
- Measure issue above

---

# Building your own evaluation set

- Start incrementally
- Use your LLM to help
- Add more data as you roll out

---

# Choosing evaluation metrics

![](/LLM_Metric.png)

---

# The role Of human evaluation

![](/Human_Evaluation.png)

---

# Test Driver Workflow for LLM

![](/Test_Driven_Workflow_for_LLM.png)