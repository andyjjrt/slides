import{b as a,o as r,w as i,g as e,ad as t,v as u,x as m,T as n}from"./modules/vue-CExxo5Y-.js";import{I as p}from"./slidev/default-D_bHMWDv.js";import{u as c,f as d}from"./slidev/context-BzMuSdlI.js";import"./index-BoNHALzr.js";import"./modules/shiki-3ci1IloS.js";const g="/long-spec/assets/method3-6asdbEwr.png",v={__name:"slides.md__slidev_10",setup(h){const{$clicksContext:l,$frontmatter:o}=c();return l.setup(),(f,s)=>(r(),a(p,u(m(n(d)(n(o),9))),{default:i(()=>[...s[0]||(s[0]=[e("h1",null,"Solution",-1),e("p",null,"Hybrid Tree Attention",-1),e("div",{class:"flex gap-4"},[e("div",{class:"grow"},[e("ul",null,[e("li",null,[e("strong",null,"The Problem:"),t(" Existing tree verification methods rely on complex masks that are incompatible with optimized kernels like Flash Attention, causing slowdowns as sequence length increases.")]),e("li",null,[e("strong",null,"The Mechanism:"),e("ul",null,[e("li",null,[e("strong",null,"Divide and Compute:"),t(" The attention computation is split into two parts: "),e("ol",null,[e("li",null,[e("strong",null,"Cached History:"),t(" Uses "),e("strong",null,"Flash Attention"),t(" for the long prefix (no masking required), ensuring high speed.")]),e("li",null,[e("strong",null,"Speculative Tokens:"),t(" Uses a custom "),e("strong",null,"Triton kernel"),t(" (fused mask attention) for the small set of new tokens.")])])]),e("li",null,[e("strong",null,"Aggregation:"),t(" The results from both computations are merged using a log-sum-exp method to ensure mathematical correctness.")]),e("li",null,[e("strong",null,"Result:"),t(" This approach reduces attention computation latency by approximately 75% compared to standard implementations.")])])])])]),e("div",null,[e("img",{src:g,class:"w-240"})])],-1)])]),_:1},16))}};export{v as default};
