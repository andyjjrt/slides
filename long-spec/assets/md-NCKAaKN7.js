import{b as a,o as r,w as i,g as e,ad as n,v as u,x as c,T as s}from"./modules/vue-CExxo5Y-.js";import{I as p}from"./slidev/default-D_bHMWDv.js";import{u as g,f as m}from"./slidev/context-BzMuSdlI.js";import"./index-BoNHALzr.js";import"./modules/shiki-3ci1IloS.js";const d="/long-spec/assets/experiment7-CcBtvn90.png",C={__name:"slides.md__slidev_15",setup(f){const{$clicksContext:o,$frontmatter:l}=g();return o.setup(),(h,t)=>(r(),a(p,u(c(s(m)(s(l),14))),{default:i(()=>[...t[0]||(t[0]=[e("h1",null,"Experiment",-1),e("p",null,"Long Reasoning Acceleration",-1),e("p",null,[n('"Reasoning" models (like QwQ) generate massive Chain-of-Thought outputs, making inference latency a critical bottleneck. Evaluated on '),e("strong",null,"QwQ-32B"),n(" using the "),e("strong",null,"AIME24"),n(" dataset with a maximum output length of "),e("strong",null,"32k tokens"),n("*.")],-1),e("ul",null,[e("li",null,[e("strong",null,"Massive Speedup:"),n(" LONGSPEC achieves a "),e("strong",null,"2.25Ã—"),n(" wall-clock speedup compared to the strong Flash Attention baseline.")]),e("li",null,[e("strong",null,"Throughput Surge:"),n(" Increases generation speed from "),e("strong",null,"18.92 tokens/s"),n(" (Vanilla) to "),e("strong",null,"42.63 tokens/s"),n(".")]),e("li",null,[e("strong",null,"High Efficiency:"),n(" Maintains a high mean accepted token count of "),e("strong",null,"3.82"),n(", proving effectiveness even on complex reasoning tasks.")])],-1),e("p",null,[e("img",{src:d,alt:"",width:"40%"})],-1)])]),_:1},16))}};export{C as default};
