import{b as i,o as r,w as a,g as e,ad as t,v as u,x as c,T as l}from"./modules/vue-CExxo5Y-.js";import{I as d}from"./slidev/default-D_bHMWDv.js";import{u as g,f as m}from"./slidev/context-BzMuSdlI.js";import"./index-BoNHALzr.js";import"./modules/shiki-3ci1IloS.js";const _={__name:"slides.md__slidev_19",setup(p){const{$clicksContext:o,$frontmatter:s}=g();return o.setup(),(f,n)=>(r(),i(d,u(c(l(m)(l(s),18))),{default:a(()=>[...n[0]||(n[0]=[e("h1",null,"Conclusion",-1),e("ul",null,[e("li",null,[e("strong",null,"Problem"),t(": "),e("ul",null,[e("li",null,"SOTA SD KV cache size grow linearly when context size grows."),e("li",null,"Models trained on short sequences fail to generalize to long contexts"),e("li",null,"Potential speedups in ingrence time")])]),e("li",null,[e("strong",null,"Solution"),t(": "),e("ul",null,[e("li",null,[e("strong",null,"Memory-Efficient Architecture:"),t(" Utilizes a constant-sized Key-Value (KV) cache via sliding window self-attention and cache-free cross-attention to eliminate memory overhead during drafting.")]),e("li",null,[e("strong",null,"Anchor-Offset Indices:"),t(" A novel training strategy that allows draft models trained on short sequences to generalize robustly to long contexts.")]),e("li",null,[e("strong",null,"Hybrid Tree Attention:"),t(" Integrates Flash Attention with custom kernels to optimize the verification step, significantly reducing attention computation latency.")])])]),e("li",null,[e("strong",null,"Experiment results"),t(": "),e("ul",null,[e("li",null,[t("Achieves up to "),e("strong",null,"3.26× speedup"),t(" over strong Flash Attention baselines on long-context understanding datasets.")]),e("li",null,[t("Delivers "),e("strong",null,"2.25× speedup"),t(" on long reasoning tasks (e.g., QwQ model on AIME24), proving effectiveness for computationally intensive logic tasks.")])])])],-1)])]),_:1},16))}};export{_ as default};
