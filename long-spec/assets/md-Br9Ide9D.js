import{b as a,o as r,w as i,g as t,ad as e,v as u,x as p,T as l}from"./modules/vue-CExxo5Y-.js";import{I as m}from"./slidev/default-D_bHMWDv.js";import{u as c,f as d}from"./slidev/context-BzMuSdlI.js";import"./index-BoNHALzr.js";import"./modules/shiki-3ci1IloS.js";const f="/long-spec/assets/experiment5-BeO0bg_9.png",b={__name:"slides.md__slidev_17",setup(h){const{$clicksContext:o,$frontmatter:s}=c();return o.setup(),(g,n)=>(r(),a(m,u(p(l(d)(l(s),16))),{default:i(()=>[...n[0]||(n[0]=[t("h1",null,"Experiment",-1),t("p",null,"Ablation Study - Hybrid Tree Attention",-1),t("p",null,"This experiment analyzed the latency breakdown of the verification step.",-1),t("ul",null,[t("li",null,[t("strong",null,"Latency Reduction:"),e(" Reduces the target modelâ€™s attention computation latency by approximately "),t("strong",null,"75%"),e(" (from ~49.92 ms to ~12.54 ms).")]),t("li",null,[t("strong",null,"The Bottleneck Fix:"),t("ul",null,[t("li",null,"Standard implementations (like EAGLE) cannot use Flash Attention for tree verification, causing slowdowns."),t("li",null,[t("strong",null,"Hybrid Tree Attention"),e(" splits the workload, allowing Flash Attention to handle the long prefix while custom kernels handle the speculation tree.")])])])],-1),t("p",null,[t("img",{src:f,alt:"",width:"60%"})],-1)])]),_:1},16))}};export{b as default};
