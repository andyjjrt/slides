import{b as s,o as r,w as i,g as t,ad as l,v as u,x as g,T as e}from"./modules/vue-CExxo5Y-.js";import{I as d}from"./slidev/default-D_bHMWDv.js";import{u as m,f as c}from"./slidev/context-BzMuSdlI.js";import"./index-BoNHALzr.js";import"./modules/shiki-3ci1IloS.js";const k={__name:"slides.md__slidev_11",setup(f){const{$clicksContext:o,$frontmatter:a}=m();return o.setup(),(p,n)=>(r(),s(d,u(g(e(c)(e(a),10))),{default:i(()=>[...n[0]||(n[0]=[t("h1",null,"Training",-1),t("ol",null,[t("li",null,[t("strong",null,"Stage 1: Short-Context Pretraining"),t("ul",null,[t("li",null,[t("strong",null,"Dataset:"),l(" SlimPajama-6B (pretraining dataset).")]),t("li",null,[t("strong",null,"Strategy:"),l(" Uses "),t("strong",null,"Anchor-Offset Indices"),l(" to simulate long-context positions. Random offsets are set between 0–15k (for 7B models) or 0–30k (for larger models).")]),t("li",null,[t("strong",null,"Goal:"),l(" Initialize the draft model and align it with the target model’s representations.")])])]),t("li",null,[t("strong",null,"Stage 2: Long-Context Adaptation"),t("ul",null,[t("li",null,[t("strong",null,"Dataset:"),l(" A small subset of the Prolong-64k dataset.")]),t("li",null,[t("strong",null,"Strategy:"),l(" Switches to "),t("strong",null,"Vanilla Indexing"),l(" since the data provides naturally long contexts.")]),t("li",null,[t("strong",null,"Goal:"),l(" Equip the model with the ability to handle extended sequences directly.")])])]),t("li",null,[t("strong",null,"Stage 3: Supervised Fine-Tuning (SFT)"),t("ul",null,[t("li",null,[t("strong",null,"Dataset:"),l(" A self-built long-context SFT dataset.")]),t("li",null,[t("strong",null,"Strategy:"),l(" Continues with Vanilla Indexing.")]),t("li",null,[t("strong",null,"Goal:"),l(" Further refine performance for instruction-following and specific tasks.")])])])],-1)])]),_:1},16))}};export{k as default};
