import{b as s,o as l,w as i,g as e,a9 as t,v as u,x as m,C as o}from"./modules/vue-j41QyVbg.js";import{I as d}from"./slidev/default-CxLWDW9T.js";import{u as c,f as p}from"./slidev/context-b2JJ8EB_.js";import"./index-BiU4ZQ3P.js";import"./modules/shiki-C01_B2YW.js";const L={__name:"slide.md__slidev_2",setup(f){const{$clicksContext:r,$frontmatter:a}=c();return r.setup(),(g,n)=>(l(),s(d,u(m(o(p)(o(a),1))),{default:i(()=>n[0]||(n[0]=[e("h1",null,"Introduction",-1),e("ul",null,[e("li",null,"Vision-Language Models (VLMs) are rapidly improving."),e("li",null,[t("Current benchmarks are "),e("strong",null,"limited scoped"),t(" and "),e("strong",null,"inconsistent in evaluation"),t(".")]),e("li",null,[t("Presents VHELM, an extension of HELM. "),e("ul",null,[e("li",null,"Aggregates various datasets to cover one or more of the 9 aspects."),e("li",null,"Standardize the inference flow to enable fair comparisons across models."),e("li",null,[t("Living benchmark ("),e("a",{href:"https://crfm.stanford.edu/helm/vhelm/v2.0.1",target:"_blank"},"https://crfm.stanford.edu/helm/vhelm/v2.0.1"),t(").")])])])],-1)])),_:1},16))}};export{L as default};
