import{o as s,c as r,k as u,e as t,q as i,s as a,B as l}from"./modules/vue-6mTy9ZQF.js";import{I as m}from"./slidev/default-B8RdL-hL.js";import{u as p,f as c}from"./slidev/context-_fRd7GMj.js";import"./index-aaSGCAS-.js";import"./modules/shiki-BzntFxfw.js";const L={__name:"slide.md__slidev_33",setup(d){const{$slidev:g,$nav:h,$clicksContext:n,$clicks:f,$page:_,$renderContext:v,$frontmatter:o}=p();return n.setup(),(x,e)=>(s(),r(m,i(a(l(c)(l(o),32))),{default:u(()=>e[0]||(e[0]=[t("h1",null,"Conclusion",-1),t("ul",null,[t("li",null,[t("strong",null,"Problem"),t("ul",null,[t("li",null,"Large KV cache"),t("li",null,"Complex decoding algorithms"),t("li",null,"Scheduling for unknown input & output lengths.")])]),t("li",null,[t("strong",null,"Solution"),t("ul",null,[t("li",null,"Proposes PagedAttention, a new attention algorithm that allows attention keys and values to be stored in non-contiguous paged memory."),t("li",null,"Presents vLLM, a high-throughput LLM serving system with efficient memory management enabled by PagedAttention.")])]),t("li",null,[t("strong",null,"Experiment"),t("ul",null,[t("li",null,"2-4Ã— throughput improvements over the SOTA systems.")])])],-1)])),_:1},16))}};export{L as default};
