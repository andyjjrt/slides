import{b as s,o as u,w as r,g as t,v as i,x as a,T as e}from"./modules/vue-CVDKllWo.js";import{I as m}from"./slidev/default-BdqQOdhu.js";import{u as p,f as g}from"./slidev/context-CVuyOCRb.js";import"./index-BroGBwX_.js";import"./modules/shiki-C7LdrR-Q.js";const P={__name:"slides.md__slidev_33",setup(d){const{$clicksContext:n,$frontmatter:o}=p();return n.setup(),(c,l)=>(u(),s(m,i(a(e(g)(e(o),32))),{default:r(()=>[...l[0]||(l[0]=[t("h1",null,"Conclusion",-1),t("ul",null,[t("li",null,[t("strong",null,"Problem"),t("ul",null,[t("li",null,"Large KV cache"),t("li",null,"Complex decoding algorithms"),t("li",null,"Scheduling for unknown input & output lengths.")])]),t("li",null,[t("strong",null,"Solution"),t("ul",null,[t("li",null,"Proposes PagedAttention, a new attention algorithm that allows attention keys and values to be stored in non-contiguous paged memory."),t("li",null,"Presents vLLM, a high-throughput LLM serving system with efficient memory management enabled by PagedAttention.")])]),t("li",null,[t("strong",null,"Experiment"),t("ul",null,[t("li",null,"2-4Ã— throughput improvements over the SOTA systems.")])])],-1)])]),_:1},16))}};export{P as default};
