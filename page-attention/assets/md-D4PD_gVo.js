import{o as s,c as i,k as l,e,a9 as t,q as u,s as d,B as o}from"./modules/vue-6mTy9ZQF.js";import{I as c}from"./slidev/default-lu_jftxV.js";import{u as m,f as p}from"./slidev/context-7S897bjf.js";import"./index-Bsy12juD.js";import"./modules/shiki-BzntFxfw.js";const B={__name:"slide.md__slidev_4",setup(f){const{$slidev:g,$nav:h,$clicksContext:a,$clicks:v,$page:_,$renderContext:L,$frontmatter:r}=m();return a.setup(),(y,n)=>(s(),i(c,u(d(o(p)(o(r),3))),{default:l(()=>n[0]||(n[0]=[e("h1",null,"Introduction",-1),e("ul",null,[e("li",null,"Identify the challenges in memory allocation in serving LLMs and quantify their impact on serving performance."),e("li",null,[t("Propose "),e("span",{color:"red"},"PagedAttention"),t(", an attention algorithm that operates on KV cache stored in non-contiguous paged memory, which is inspired by the virtual memory and paging in OS.")]),e("li",null,[t("Design and implement "),e("span",{color:"red"},"vLLM"),t(", a distributed LLM serving engine built on top of PagedAttention.")]),e("li",null,"Evaluate vLLM on various scenarios and demonstrate that it substantially outperforms the previous state-of-the art solutions such as FasterTransformer and Orca")],-1)])),_:1},16))}};export{B as default};
