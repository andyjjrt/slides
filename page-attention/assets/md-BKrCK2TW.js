import{b as s,o as i,w as l,g as t,ad as e,v as u,x as d,T as n}from"./modules/vue-CVDKllWo.js";import{I as m}from"./slidev/default-BdqQOdhu.js";import{u as p,f as c}from"./slidev/context-CVuyOCRb.js";import"./index-BroGBwX_.js";import"./modules/shiki-C7LdrR-Q.js";const x={__name:"slides.md__slidev_4",setup(f){const{$clicksContext:a,$frontmatter:r}=p();return a.setup(),(g,o)=>(i(),s(m,u(d(n(c)(n(r),3))),{default:l(()=>[...o[0]||(o[0]=[t("h1",null,"Introduction",-1),t("ul",null,[t("li",null,"Identify the challenges in memory allocation in serving LLMs and quantify their impact on serving performance."),t("li",null,[e("Propose "),t("span",{color:"red"},"PagedAttention"),e(", an attention algorithm that operates on KV cache stored in non-contiguous paged memory, which is inspired by the virtual memory and paging in OS.")]),t("li",null,[e("Design and implement "),t("span",{color:"red"},"vLLM"),e(", a distributed LLM serving engine built on top of PagedAttention.")]),t("li",null,"Evaluate vLLM on various scenarios and demonstrate that it substantially outperforms the previous state-of-the art solutions such as FasterTransformer and Orca")],-1)])]),_:1},16))}};export{x as default};
